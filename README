###### README ########


################################
#### 1. Getting Started ########
################################


- HOW TO RUN THIS APPLICATION:

You can run the application by running docker compose, which starts the server and all neccessities, by writing in a terminal (in the same directory as this file):

sudo allowed_hosts=['*'] cel_url='amqp://admin:mypass@rabbit:5672' docker-compose up

instead of ['*'], which allows all hosts, you can write a number of the port that django is using, for example ['0.0.0.0']. It is important to specify (even if it is *, meaning all addresses) allowed hosts since otherwise Django will not be able to show the website to you. The cel_url parameter should be the same on all computers but it is possible to specify password and username (which will be described in 3. How the code works).

You can find out which port Django is using by running

sudo allowed_hosts=['*'] cel_url='amqp://admin:mypass@rabbit:5672' docker-compose up

and waiting until a line appears in the terminal that says

clust_app     | Starting development server at http://0.0.0.0:8000/

In this case, the address where the server runs is 0.0.0.0. This is the address you need later to access the site.

IN ADDITION, in order to allow the live updates of algorithm progress, open a terminal in the same directory as this file (not on the Docker container, but on your local computer) and type:

python manage.py livereload

This gets the server running. You can then access the website by typing:

http://0.0.0.0:8000/clustering/clustering_6_4.html

And substitute 0.0.0.0:8000 by the address and port number that is printed out by the application as above.

#################################################
##### 2. How to use the web application #########
#################################################

To run a bi-clustering analysis on the web application, you can use either built-in datasets or your own data. You must upload or select data by clicking through a multi-step form.
To make an example run, click the big button that says "run new analysis" and then check the checkbox that says "use pre-compiled data". Select one of the possibilites in the dropdown menu (it selects expression datasets).
On the next step, click "import ndex" and select an option from the dropdown menu (it contains PPI networks for the analysis). Skip the third step. On the fourth step, type "10" in the first field and "20" in the second field. This selects a target gene number of 10 to 20 for the resulting networks. Click submit and the analysis is run. You will see a progress image that tells you the current progress of your analysis. If it says "done", scroll down to view the tabs with results. You can select which part of the result to show by selecting from the big clickable bar.

Own data must consist of one file with expression data in CSV or TSV format (the exact format is visible on the website), a file with PPI consisting of two columns with interaction partners, and optionally of clinical patient data in CSV format. If you want to analyze survival of patients, you must indicate which column of your clinical data file lists the survival time of patients.

You can change the exact settings for the algorithm in the fourth tab.

To run an enrichment analysis, scroll down to the results and select the tab "run enrichment". 
 


#################################################
##### 3. How the code works #####################
#################################################

################
What runs where:
################

There are 4 docker containers that each run a task or a server that is necessary for the web application:

- Livereload: A python-based application/library for Django needed for a good display of the loading page. Livereload makes every file reload automatically when it is changed.
There appears to still be a bug with livereload on Docker, so it is possible that this part of the server will be replaced by javascript code with the same functionality.

- RabbitMQ: Needed as server where celery executes its tasks. Must run in the background for celery to run properly.

- Celery: Used as queuing system to execute tasks synchronously. Executes all computationally intensive tasks such as running the algorithm or processing results.

- Django: The server for running and processing requests. This container is where the actual server runs and where paths to files can be referenced.

There is a shared volume that can be accessed by all containers under /code. This is needed to transfer files from one to another container by the server.

In order to show the progress of the algorithm while running, LiveReload is needed to run in both the container and the folder where the container is started. For this, open two tabs in the terminal, in the first run: 
python manage.py livereload
In the second:
sudo docker compose-up
This ensures that everytime a gif or png changes, which is needed to display the running status of the algorithm, it is reloaded in the page displayed to the user.

In addition to the Docker containers, a crontab runs at /etc/crontab to automatically delete old unused user files (files with results, e.g heatmap png).


###############################
What is where (in the code):
###############################

In docker-compose.yml, general specifications are listed for the different parts of the application. There you can specify a different password for RabbitMQ by modifying the line RABBITMQ_DEFAULT_USER.
You can change the port on which the website is running by modifying the command
 python manage.py runserver 0.0.0.0:8000 
and substituting 0.0.0.0 by another address.

Most of the settings for the server are in settings.py, such as the communication between Django, Celery and RabbitMQ. There are comments which line is used to do what, and what must be modified to change a specific setting.

Templates (HTML pages visible to the user) are connected in a file called "urls.py" with a function that processes the request for each template.

Files with results are stored in a static directory for which the location is defined in settings.py. This static directory is located in a shared volume (path is "/code") that is accessible for all containers (celery, livereload, django, rabbitmq). This is important because the celery container manipulates data that must be visible to the django container. 

The code needed for running a request is in three different files:
- views.py - processes the request, will be described below.
- tasks.py - running the algorithm on the queuing system and pre/postprocessing files
- weighted_aco_lib - parts of the algorithm that are connected to a method in tasks.py

#####################################
How a request runs on the server:
#####################################

If you submit a request, several steps are executed in views.py. 
For the loading page, the gif with loading symbol is copied to its place so that it is visible in the page. The current status of the run is written and read from a log file.
Data are preprocessed by preprocess_file in tasks.py so that CSV files are converted to TSV. There are lines of code in this script that, when uncommented, automatically select the two biggest clusters if more than two clinical patient groups exist in the expression file.
Two scripts are used to run the algorithm for the data in tasks.py: algo_output_task and script_output_task_10. These tasks are executed via Celery, and their results are written on the shared volume (under /code).

#####################################
How the templates work:
#####################################

The results are referenced in the templates by setting django variables (in the views.py script) with paths to json files for PPI data and images for the other plots.
Two javascript libraries are used for the templates (they are included in the Docker container), sigma.js for displaying PPI graphs and dataTable.js for a good display of enrichment results in tables.

#####################################
Which methods do what:
#####################################

In views.py, methods are defined that run code after the user accesses a web page.


#####################################
#### 4. Troubleshooting: ############
#####################################

Some Errors that might occur:

- Celery does not start properly and RabbitMQ produces an error upon starting:

After usable space on the server was totally full, this error can occur. It is possibly solved by typing
 sudo rabbitmqctl stop
 sudo rabbitmq-server

If this does not help, then a log file of the RabbitMQ server was corrupted. This is no big deal and can be solved by running
 mkdir -p /tmp/badrabbit/ 
 sudo mv /var/lib/rabbitmq/mnesia/rabbit[[[COMPUTER_NAME]]]/queues/* /tmp/badrabbit/;
substitute COMPUTER_NAME by the name of your computer/the server in the shell, that is shown in the terminal at the start of each line.

- RabbitMQ does not do anything after running a request:

If this happens, look for a line in the output of RabbitMQ that says that the memory is insufficient. RabbitMQ needs at least 50 MB of free memory on the system, otherwise it will not take any requests from Celery. 

- The database gives a "duplicate table" error on the shell:
e.g. "psycopg2.errors.DuplicateTable: relation "clustering_graphform" already exists"
This is a database error that is produced because there are two containers running postgres. This error can happen if you accidentally built the postgres contatiner twice. Delete the older postgres
container to avoid this error.



